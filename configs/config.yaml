# TEP Data Processing Pipeline Configuration

# Data Paths
data:
  raw_source: 'C:\path\to\TEP_Mode1.h5'  # UPDATE THIS: Path to your 23GB source file
  output_dir: 'C:\path\to\output'        # UPDATE THIS: Where to save processed data

# Random Seed
random_seed: 42

# Feature Engineering Settings
feature_engineering:
  drop_analyzers: true        # Drop analyzer measurement features
  drop_zero_variance: true    # Drop features with zero variance
  variance_threshold: 0.0     # Threshold for zero variance detection
  
  # IMPORTANT: List the EXACT column names you want to drop
  # Run: python scripts/inspect_columns.py "path/to/TEP_Mode1.h5"
  # to see all your column names, then copy the ones you want to drop here
  
  analyzer_features:
    # Example - Replace these with YOUR actual column names:
    # - 'Component A'
    # - 'Component B' 
    # - 'Component C'
    # - 'Reactor Comp D'
    # - 'Separator Comp E'
    
    # For now, this is empty - Fill this  after running inspect_columns.py
    ['Time', 
     'Component A to Reactor',
     'Component B to Reactor',
     'Component C to Reactor',
     'Component D to Reactor',
     'Component E to Reactor',
     'Component F to Reactor',

     'Component A in Purge',
     'Component B in Purge',
     'Component C in Purge',
     'Component D in Purge',
     'Component E in Purge',
     'Component F in Purge',
     'Component G in Purge',
     'Component H in Purge',

     'Component D in Product',
     'Component E in Product',
     'Component F in Product',
     'Component G in Product',
     'Component H in Product',

     'A in stream 1',
     'B in stream 1',
     'C in stream 1',
     'D in stream 1',
     'E in stream 1',
     'F in stream 1',

     'A in stream 2',
     'B in stream 2',
     'C in stream 2',
     'D in stream 2',
     'E in stream 2',
     'F in stream 2',

     'A in stream 3',
     'B in stream 3',
     'C in stream 3',
     'D in stream 3',
     'E in stream 3',
     'F in stream 3',
     
     'A in stream 4',
     'B in stream 4',
     'C in stream 4',
     'D in stream 4',
     'E in stream 4',
     'F in stream 4']

# Data Saving Options
save_options:
  save_dataframes: true       # Save final DataFrames as .pkl files
  save_csv_sample: true       # Save CSV sample for inspection
  csv_sample_rows: 10000      # Number of rows in CSV sample

# Split Configuration (runs per split)
splits:
  total_runs: 50              # Runs to select per IDV
  train_runs: 30              # Runs for training
  val_runs: 10                # Runs for validation
  test_runs: 10               # Runs for testing

# Scaling Options
scaling:
  method: 'standard'          # Options: 'standard', 'minmax', 'robust'
  keep_unscaled: ['Time']     # Columns to keep unscaled